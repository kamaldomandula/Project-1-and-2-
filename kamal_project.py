# -*- coding: utf-8 -*-
"""kamal project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s22RQLHg40btUhfCo0R5yoiVWtnA42PN
"""

# Reorganized import statements and added explanations
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Load and preview dataset
data = pd.read_csv('telco-customer-churn.csv')
data.head()  # Preview the data

# Initial data check: identifying missing values
missing_info = data.isnull().sum()
print('Columns with missing values:', missing_info[missing_info > 0])

# Replacing any blank spaces in the dataset with NaN for cleaner data
data.replace(' ', np.nan, inplace=True)

# Outlier visualization via boxplots for numeric columns
numerics = data.select_dtypes(include=['float64', 'int64']).columns
for column in numerics:
    plt.figure(figsize=(9, 4))
    sns.boxplot(x=data[column], color='skyblue')
    plt.title(f'Outlier Detection for {column}')
    plt.show()

# Label encoding for categorical features
encoder = LabelEncoder()
for category in data.select_dtypes(include=['object']).columns:
    data[category] = encoder.fit_transform(data[category])

# Standardizing numeric columns for uniform feature scaling
scaler = StandardScaler()
data[numerics] = scaler.fit_transform(data[numerics])

# Overview of the cleaned and transformed dataset
data.info()

# Data types and column grouping for reference
print('Data Type Summary:')
print(data.dtypes.value_counts())
display('\nCategorical columns:', data.select_dtypes(include=['object']).columns.tolist())
display('Numerical columns:', data.select_dtypes(include=['float64', 'int64']).columns.tolist())

# Correlation matrix to analyze feature relationships, using mask for upper triangle
plt.figure(figsize=(12, 10))
correlation_matrix = data.corr()

# Mask to hide upper triangle for a cleaner look
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Create the heatmap with custom color map and mask
sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='YlGnBu', fmt='.2f', square=True,
            linewidths=0.5, cbar_kws={"shrink": 0.75})
plt.title('Feature Correlation Matrix', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.show()

# Feature importance analysis with Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
X_features = data.drop('Churn', axis=1)
y_target = data['Churn']
forest_model = RandomForestClassifier(random_state=0)
forest_model.fit(X_features, y_target)
importance_df = pd.DataFrame({'Feature': X_features.columns, 'Importance': forest_model.feature_importances_})
importance_df.sort_values(by='Importance', ascending=False, inplace=True)
print('Top Important Features:')
importance_df.head(10)

# Plotting top features by importance with adjusted syntax
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), hue='Feature', dodge=False, palette='viridis', legend=False)
plt.title('Top 10 Important Features')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

# Train-test split and SweetViz for comparative analysis
import sweetviz as sv

X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=42)

report = sv.compare([X_train, 'Train Set'], [X_test, 'Test Set'])
report.show_html('comparison_report.html')

# Key limitations and considerations
print('Dataset Insights and Limitations:')
print('1. Class imbalance may impact model performance and necessitate resampling methods.')
print('2. Scaling applied to ensure compatibility across different algorithms.')
print('3. Consider using SMOTE or similar techniques for addressing imbalances in target classes.')

