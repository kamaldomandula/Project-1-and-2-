# -*- coding: utf-8 -*-
"""Krystn project 1 (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bm8VYJjRXekn5zbPy1eV-tzLtGxlXV4G

**1: Importing of Libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""**2: Extracting the Updated/Augmented Automobile Data File**"""

Dataset = "Desktop/telco-customer-churn.csv"
dataset = pd.read_csv('telco-customer-churn.csv')

dataset.head(5)

dataset.tail(5)

"""**3: Data Wrangling**

1. Look for missing, null, NaN records.
     Find outliers. Transform data – all entries should be numeric.
"""

#Replace the '?' in the data file with Nan
dataset.replace("?",np.NaN, inplace=True)

#Calculating the number of missing data in each column and printing the same
dataset.isnull().sum()
for column in dataset.columns:
    print(column)
    print(dataset[column].isnull().sum())

# Identify numeric columns
numeric_columns = dataset.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Identify categorical columns (usually object type in pandas)
categorical_columns = dataset.select_dtypes(include=['object']).columns.tolist()

# If there are any text columns (which are also often 'object'), you can refine them here
# Check if any categorical columns are actually text
text_columns = [col for col in categorical_columns if dataset[col].str.contains('[A-Za-z]', na=False).any()]

# Now remove text columns from categorical
categorical_columns = [col for col in categorical_columns if col not in text_columns]

# Display the lists
print("Numeric Columns:", numeric_columns)
print("Categorical Columns:", categorical_columns)
print("Text Columns:", text_columns)

"""2. List all types of data, numeric, categorical, text."""

#Checking the data types for each column
dataset.dtypes

# List of binary columns to convert
binary_columns = ['Churn', 'gender']  # Adjust based on your actual column names

# Replace values in binary columns
dataset[binary_columns] = dataset[binary_columns].replace({'Yes': 1, 'No': 0, 'Male': 0, 'Female': 1})

# Explicitly downcast to retain behavior (optional)
dataset[binary_columns] = dataset[binary_columns].infer_objects(copy=False)

# Check the result
print(dataset.head())

# Convert multi-class categorical columns to numeric codes
categorical_columns = ['InternetService', 'Contract', 'PaymentMethod', 'MultipleLines',
                       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',
                       'StreamingTV', 'StreamingMovies']
for column in categorical_columns:
    dataset[column] = dataset[column].astype('category').cat.codes

# Convert 'TotalCharges' to numeric
dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'], errors='coerce')

# Check the data types
print(dataset.dtypes)

dataset.head(5)

dataset.tail(5)

for column in categorical_columns:
    dataset[column] = dataset[column].astype('category')

print(dataset.dtypes)

# Identify numeric columns
numeric_columns = dataset.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Identify categorical columns (usually object type in pandas)
categorical_columns = dataset.select_dtypes(include=['category']).columns.tolist()

# If there are any text columns (which are also often 'object')
text_columns = dataset.select_dtypes(include=['object']).columns.tolist()

# Display the lists
print("Numeric Columns:", numeric_columns)
print("Categorical Columns:", categorical_columns)
print("Text Columns:", text_columns)

import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

"""3.Present dependencies and correlations among the various features in the data"""

# Example: Assuming 'Gender' is a categorical variable in your dataset
sns.countplot(data=dataset, x='Churn', hue='gender', palette='Set2')
plt.title('Churn Count by Gender')
plt.show()

data_numeric = dataset.drop(columns=['customerID'])  # Drop non-numeric identifiers

# Select only the numeric columns from the dataset
data_numeric = dataset.select_dtypes(include=[np.number])

# Calculate the correlation matrix
corr = data_numeric.corr()
# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Calculate the correlation coefficients with 'Churn'
correlation_with_churn = corr['Churn'].sort_values(ascending=False)
print("Correlation with Churn:")
print(correlation_with_churn)

"""**Positive Correlations: MonthlyCharges: 0.193; PaperlessBilling: 0.192; SeniorCitizen: 0.151**

4. Split Dataset
"""

pip install sweetviz

from sklearn.model_selection import train_test_split
import sweetviz as sv

# Define the features (X) and the target (y)
X = dataset.drop(columns=['Churn'])  # Drop the 'Churn' column
y = dataset['Churn']  # Target variable

# Split the dataset into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Combine the training and test data back with their respective labels for SweetViz
train_data = X_train.copy()
train_data['Churn'] = y_train

test_data = X_test.copy()
test_data['Churn'] = y_test

# Create the comparison report using SweetViz
comparison_report = sv.compare([train_data, "Training Set"], [test_data, "Test Set"], "Churn")

# Display the report in an HTML file
comparison_report.show_html("sweetviz_comparison_report.html")

"""Pandas Profiling"""

!pip install pandas-profiling
!pip install visions==0.7.4

!pip install ydata-profiling
!pip install --upgrade numba
!pip install --upgrade ydata-profiling visions
!pip uninstall ydata-profiling visions numba
!pip install ydata-profiling visions numba

from ydata_profiling import ProfileReport

# Create the profile report
profile = ProfileReport(dataset, title="Telco Customer Churn - Pandas Profiling Report", explorative=True)

# Save the report as an HTML file
profile.to_file("ydata_profiling_report.html")

"""5) State limitations/issues if any with the given dataset.

Challenges in Churn Prediction Analysis

1. Class Imbalance in Target Variable (Churn)
Description: The dataset displays a notable imbalance between the number of customers who churn and those who stay. Typically, the number of non-churning customers far exceeds that of churners.

Consequences: This imbalance can lead to biased predictive models that favor the majority class (non-churners), resulting in decreased performance in accurately predicting the minority class (churners). Consequently, the model may struggle to identify and address customer churn effectively.

2. Presence of Categorical Variables
Description: Many features in the dataset, such as InternetService, Contract, and PaymentMethod, are categorical. For machine learning algorithms to process these features, they must be converted into numeric representations.

Consequences: The transformation process can introduce complexities, such as the risk of multicollinearity or the potential loss of information during encoding (e.g., one-hot encoding or label encoding). These issues may hinder the overall performance of the model.

3. Potential Redundancy and Low Variability in Features
Description: Certain features, including PhoneService and MultipleLines, might show very low variability. For instance, a large majority of customers may have a phone service, resulting in limited diversity in the data.

Consequences: Features with minimal variability often contribute little to the model’s predictive power regarding churn. Their presence could introduce noise, complicating the model unnecessarily. Thus, considering the removal of such features may enhance model efficiency and interpretability.
"""

